{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IG-Reagan/Bank-of-England_Cambridge-ICE_NLP_Analysing-Quarterly-Announcements-of-GSIBs/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jYKPfuMqhDca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path = '/content/drive/MyDrive/QA_analysis_HybrRAG_Mistral_JPMC_all_original_.xlsx'\n",
        "\n",
        "if os.path.exists(path):\n",
        "    try:\n",
        "        # Read the Excel file into a pandas DataFrame\n",
        "        df = pd.read_excel(path)\n",
        "\n",
        "        # Convert the DataFrame to a CSV string\n",
        "        csv_data = df.to_csv(index=False)  # Set index=False to avoid writing row indices\n",
        "\n",
        "        # Define the output CSV file path\n",
        "        csv_file_path = '/content/drive/MyDrive/QA_analysis_HybrRAG_Mistral_JPMC_all_original_.csv'\n",
        "\n",
        "        # Write the CSV data to a file\n",
        "        with open(csv_file_path, 'w') as f:\n",
        "            f.write(csv_data)\n",
        "\n",
        "        print(f\"Excel file successfully converted to CSV at: {csv_file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\"File not found at {path}\")"
      ],
      "metadata": {
        "id": "HbZbOOh9fOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ke10en6Czmgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EJ6AI13wipHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer"
      ],
      "metadata": {
        "id": "1R0-oPxAqurU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "kDBoJjEjox04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and sentiment analysis pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")"
      ],
      "metadata": {
        "id": "n0E-mCzs1cqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to chunk text into max 512-token pieces\n",
        "def chunk_text(text, max_tokens=512, overlap_ratio=0.1):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return [text]  # Return as is if within limit\n",
        "\n",
        "    # Convert back to text using sentence tokenization\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks, current_chunk = [], []\n",
        "\n",
        "    token_count = 0\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        if token_count + len(sentence_tokens) > max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = current_chunk[-int(len(current_chunk) * overlap_ratio):]  # Overlap\n",
        "            token_count = sum(len(tokenizer.encode(s, add_special_tokens=False)) for s in current_chunk)\n",
        "\n",
        "        current_chunk.append(sentence)\n",
        "        token_count += len(sentence_tokens)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "DTC2urNNmXFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute sentiment score for chunks and aggregate\n",
        "def analyze_sentiment(text):\n",
        "    chunks = chunk_text(text)\n",
        "    sentiment_scores = []\n",
        "    sentiment_labels = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        result = classifier(chunk)[0]\n",
        "        sentiment_scores.append(result[\"score\"])\n",
        "        sentiment_labels.append(result[\"label\"])\n",
        "\n",
        "    # Aggregation\n",
        "    avg_score = np.mean(sentiment_scores)  # Average sentiment score\n",
        "    final_label = max(set(sentiment_labels), key=sentiment_labels.count)  # Majority vote for final sentiment\n",
        "\n",
        "    return final_label, avg_score"
      ],
      "metadata": {
        "id": "zfFfvf5LqkGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through the DataFrame and perform sentiment analysis\n",
        "for index, row in df.iterrows():\n",
        "    try:\n",
        "        # Process Question\n",
        "        question_label, question_score = analyze_sentiment(row['question'])\n",
        "        df.at[index, 'question_sentiment'] = question_label\n",
        "        df.at[index, 'question_sentiment_score'] = question_score\n",
        "\n",
        "        # Process Answer\n",
        "        answer_label, answer_score = analyze_sentiment(row['answer'])\n",
        "        df.at[index, 'answer_sentiment'] = answer_label\n",
        "        df.at[index, 'answer_sentiment_score'] = answer_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing row {index}: {e}\")\n",
        "        df.at[index, 'question_sentiment'] = \"Error\"\n",
        "        df.at[index, 'question_sentiment_score'] = -1\n",
        "        df.at[index, 'answer_sentiment'] = \"Error\"\n",
        "        df.at[index, 'answer_sentiment_score'] = -1\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "3guRG_ahiJNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "d3upTo-7hxW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert df into xlsx file\n",
        "\n",
        "# Convert the DataFrame to an xlsx file\n",
        "output_xlsx_path = '/content/drive/MyDrive/QA_analysis_HybrRAG_Mistral_JPMC_all_original_with_sentiment_analysis.xlsx'\n",
        "df.to_excel(output_xlsx_path, index=False)  # Set index=False to avoid writing row indices\n",
        "\n",
        "print(f\"DataFrame successfully converted to xlsx at: {output_xlsx_path}\")\n"
      ],
      "metadata": {
        "id": "DnVRLR9Y3Z9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: count the different results from the question_sentiment and answer_sentiment columns in df\n",
        "\n",
        "# Count different results in 'question_sentiment' and 'answer_sentiment' columns\n",
        "question_sentiment_counts = df['question_sentiment'].value_counts()\n",
        "answer_sentiment_counts = df['answer_sentiment'].value_counts()\n",
        "\n",
        "print(\"Question Sentiment Counts:\\n\", question_sentiment_counts)\n",
        "print(\"\\nAnswer Sentiment Counts:\\n\", answer_sentiment_counts)\n"
      ],
      "metadata": {
        "id": "uXGTdDvmxOJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Count the occurrences of each sentiment label\n",
        "sentiment_counts = df['question_sentiment'].value_counts()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.title('Distribution of Question Sentiments')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Number of Questions')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A0j_4-llhx6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each sentiment label for answers\n",
        "sentiment_counts_answer = df['answer_sentiment'].value_counts()\n",
        "\n",
        "# Create the bar chart for answer sentiments\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sentiment_counts_answer.index, y=sentiment_counts_answer.values)\n",
        "plt.title('Distribution of Answer Sentiments')\n",
        "plt.xlabel('Sentiment Label')\n",
        "plt.ylabel('Number of Answers')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i5ujBxC9iwxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}